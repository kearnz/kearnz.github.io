# Findings {#findings .unnumbered}

## Full Model Recap {.unnumbered}

This section begins with a recap of linear regression on the full dataset. The results below serve as the benchmark for comparison and are considered the golden standard.

The results for the full model are displayed again:

```{r full-regression-again, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Recap: Full"}
include_graphics(path = "figure/full-regression.jpg")
```

Next, the authors examine whether the results from the missing data methods distort the results from linear regression on imputed data. If coefficients are biased, then the missing data method used does not properly respect the original structure of the complete data. Further, if variance measures are too narrow, then the missing data method does not create sufficient variability to account for uncertainty introduced by missingness, and the statistical significance of the coefficients may be called into question.

The authors display the results of linear regression for each example in the methodology section.

## Results from Example 1  {.unnumbered}
 
The authors review the results from linear regression fit to the multiply imputed data from Example 1. Recall that Example 1 contains 40% missingness in the response $y$, and the missingness mechanism is MCAR.

First, listwise delete:

```{r mcar-listwise-delete, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MCAR: Listwise Delete"}
include_graphics(path = "figure/mcar-listwise-delete.jpg")
```

Next, mean imputation:

```{r mcar-mean, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MCAR: Mean Imputation"}
include_graphics(path = "figure/mcar-mean.jpg")
```

Next, least squares imputation:

```{r mcar-ls, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MCAR: Least Squares Imputation"}
include_graphics(path = "figure/mcar-ls.jpg")
```

Finally, pmm imputation:

```{r mcar-pmm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MCAR: PMM Imputation"}
include_graphics(path = "figure/mcar-pmm.jpg")
```

## Findings from Example 1 {.unnumbered}

The authors explore the impact each missing data method has on the linear regression's coefficient estimate for $x$ and the resulting standard error and variance of $x$. 

First, the authors discuss coefficient estimates. Mean imputation produces a grossly biased parameter estimate for $x$. This result occurs because mean imputation thins the distribution of the variable it imputes. As a result, mean imputation shrinks the correlation between any feature and the imputed variable (van Buuren, ch. 1.3.1). When the imputed variable is the response in a linear regression, the resulting coefficient trends toward zero. Therefore, mean imputation is suboptimal in this case. 

Listwise deletion, least squares, and PMM all produce parameter estimates close to the true value of the parameter estimate from the regression model on the complete dataset. PMM's estimate is the least biased of the three. These results are also expected. Under MCAR with missingness in the response only, listwise deletion, least squares, and PMM all produce unbiased estimates for the parameter of interest (Van Buuren, ch. 1.3.8; ch 3.4). 

The small bias in this example stems from the fact that the researchers apply this process to only one randomly generated dataset. While out of scope for this study, a researcher could conduct a monte carlo simulation, repeating the process in Example 1 for thousands datasets generated using the same multivariate normal distribution and averaging the bias from each simulation. Such a study would produce a distribution for bias itself, and that distribution should be centered around 0, indicating that each imputation method leads to unbiased parameter estimates on average in analysis models. 

Next, the authors examine the impact of imputation methods on the variance of parameter estimates. Mean imputation produces similar standard error and variance as the full regression model, because mean imputation does not produce any variability between imputations. Results from least squares are even worse. The standard error and variance of coefficient estimates shrink significantly. This phenomenon stems from the fact that least squares imputation increases the correlation between features and the imputed variable. As a result, the variability of the imputed data is understated, and the resulting standard error of the coefficient estimate is far too low. This underestimation makes the coefficient seem more statistically significant than it may be. 

Listwise deletion produces larger variance and standard error because the sample size used for the regression model is reduced. PMM imputation, on the other hand, increases the variance and standard error of the coefficient estimates for a different reason than listwise deletion. Notice that PMM is the only method that produces a non-zero value for the variance-between. PMM produces different parameter estimates in each analysis model applied to each of the multiply imputed datasets. Therefore, when the coefficients are pooled, additional variance results between each model's coefficient estimates. This additional variance captures the uncertainty introduced from imputing missing values, given that the true value is not known and actually comes from a distribution itself. 

## Results from Example 2  {.unnumbered}

Next, the authors review the results from a linear regression on the imputed datasets from Example 2. Recall that Example 2 contains 40% missingness in the predictor $x$, and the missingness mechanism is now MAR.

First, listwise delete:

```{r mar-listwise-delete, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MAR: Listwise Delete"}
include_graphics(path = "figure/mar-listwise-delete.jpg")
```

Next, mean imputation:

```{r mar-mean, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MAR: Mean Imputation"}
include_graphics(path = "figure/mar-mean.jpg")
```

Next, least squares imputation:

```{r mar-ls, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MAR: Least Squares Imputation"}
include_graphics(path = "figure/mar-ls.jpg")
```

Finally, pmm imputation:

```{r mar-pmm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results under MAR: PMM Imputation"}
include_graphics(path = "figure/mar-pmm.jpg")
```

## Findings from Example 2 {.unnumbered}

In example 2, the missing data mechanism is MAR and right-tailed. Therefore, the probability of missingness in $x$ increases with larger values of $y$. The bias in mean imputation improves because $x$ is a predictor. That being said, the authors have already addressed why the method is suboptimal because of its disregard for the relationships between variables. Therefore, the authors focus on the results from the other missing data methods instead. 

First, note that the coefficient estimates from listwise deletion are now negatively biased. When more predictors are missing for higher values of $y$, then the underlying dataset's correlation subsides, distorting the true relationship between $x$ and $y$. For right-tailed MAR, listwise deletion ends up producing negatively biased estimates as a result.

Least squares regression, on the other hand, produces a coefficient that is grossly overbiased. This result occurs because the error distribution of the least squares imputation model is not a normal distribution with $\mu=0$, as the imputation model's predictions are postively biased. Under right-tailed MAR with missingness in the predictor, least squares imputation will always lead to postively biased coefficient estimates. This phenomenom is best explained by comparing the scatterplot for least squares imputations found in Example 1 and Example 2.

PMM is the only method that produces an unbiased estimate for the coefficient. PMM produces imputations that follow the missing data mechanism at hand, and thus it preserves the relationship between $x$ and $y$. The analysis model then performs well, and the coefficients are unbiased.

The analysis of variance is similar to example 1. In general, listwise deletion prior to analysis always enlarges the coefficient variance and standard error because the sample size is reduced. Mean imputation ignores any variability between imputations, and least squares inflates correlation between features, thus deflating the standard error and variance of a parameter estimate. PMM is the only method that retains the variability introduced from multiple imputation, and thus it increases standard error to account for the underlying uncertainty in the true value of missing data points.

## Inference from Examples {.unnumbered}

Whenever a researcher fits a supervised machine learning model of imputed data, he or she should be concerned with potential bias of the coefficient estimates and possible understatement or coefficient standard error. The examples above show that bias and standard error are the direct result of how imputation methods respond to the nature of missing data and if they handle the missing data mechanism correctly. 

The researcher should always select the method that produces the best results from the analysis model. The best results stem from an imputation method that preserves the covariance structure of the underlying data. Therefore, in Example 2, the researcher should feel comfortable selecting PMM as the best missing data method to use. In the case where numerous imputation methods lead to unbiased parameter estimates and sufficient standard error for coefficients, the researcher must then consider other factors related to the underlying missing data method. In the MCAR example, both PMM and listwise deletion do a good job. But PMM is computationally expensive, and the results from PMM vary each time the researcher imputes the data (assuming no seed is set). In this case, listwise deletion may actually be prefered. 

These examples demonstrate that there is no free lunch when it comes to analysis of imputed data. The researcher must consider multiple imputation models and assess how they affect bias and variance of coefficients. Additionally, the researcher must consider the computational cost of the underlying imputation algorithm and its flexibility to extend to other scenarios with missing data. Some imputation methods can always be rejected, such as mean and least squares imputation. Other methods, however, must be vetted and selected with careful consideration.
