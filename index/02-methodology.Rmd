# Methodology {#methodology .unnumbered}

The missing data methods introduced in the background section are a subset of those available in `Autoimpute`. Beyond `Autoimpute` numerous additional missing data methods exist to handle missing data, and they are continuously being developed and improved (Schouten, et al, 2018). Therefore, it is vital to understand under which circumstances each imputation technique can be used, and it is important to evaluate each technique in relation to other methods (Schouten, et al, 2018). To do so, researchers can simulate complete datasets and introduce different types of missing data mechanisms and missing data patterns. Such a procedure affords the researcher an opportunity to evaluate imputation methods and their affect on analysis models under different circumstances (Schouten, et al, 2018).

In this research, the authors explore the performance of imputation methods and their effect on analysis models under two separate circumstances. The authors begin by simulating a dataset with no missingness. A simple linear regression is performed on this simulated dataset and its coefficients are stored as benchmarks. Next, the researchers impose two different types of missingness mechanisms and missing data patterns on the complete dataset. This procedure produces two disctinct, incomplete datasets, where the remaining observed values are the same as the original but now some observations are missing. The authors then impose the same deletion and impution methods on each of the incomplete datasets using the multiple imputation framework. Finally, the authors apply the same linear regression model to the multiply imputed data and pool the parameter estimates and variance. The authors then compare the results from each process to each other and to the results from linear regression on the original complete dataset. The authors perform the entire process using the `Autoimpute` package, which demonstrates its capabilities as an end-to-end framework for handling missing data.

In each example, the researchers then explore missingness patterns within the dataset. Then, the authors use the following missing data methods:

* Listwise deletion or CCA
* Mean imputation
* Least Squares imputation
* Predictive Mean Matching

`Autoimpute` offers many more missing data methods, but these four are useful for examination because each carries a different set of assumptions and takes a very different approach to imputing missing values. CCA is a deletion method, so it ignores missingness entirely by discarding records with any missing observations. Mean imputation is a univariate method, so it disregards any structure between the target variable and other features. Least squares is a popular supervised multivariate method familiar to readers. And PMM is an advanced semi-supervised multivariate method that blends concepts from bayesian analysis, linear regression, and k-nearest neighbors. The sections below demonstrate when these methods work and when they do not. As always, their performance is directly linked to the nature of missingness within a dataset. Therefore, some methods shine in circumstances where other methods underperform.

## The Full Dataset {.unnumbered}

The authors begin by simulating the full dataset, which they use as the source of truth. The full dataset contains 500 observations for feature $x$ and response $y$. At this point, no missingness exists in either $x$ or $y$

The figure below visualizes the distribution of each variable:

```{r fullsidebyside, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Distribution of Full x and y"}
include_graphics(path = "figure/full-side-by-side.jpg")
```

The next figure displays the joint relationship between $x$ and $y$. The figure plots the marginal distribution of each variable as well:

```{r full-joint, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Joint and Marginals of Full x and y"}
include_graphics(path = "figure/full-joint.jpg")
```

The full dataset does not contain any missing values. Therefore, we do not have to perform any imputation before we conduct analysis. Using `Autoimpute`, the researchers fit a linear regression model on the full dataset. The summary of the model fit appears below:

```{r full-regression, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Linear Regression Results: Full"}
include_graphics(path = "figure/full-regression.jpg")
```

The results from linear regression on the full dataset serve as the golden source. The coefficient estimate for $x$ is 0.5. Using notation related to imputation analysis, this coefficient estimate of $x$ represents $\hat Q$ - the unbiased estimate of the population parameter of interest $Q$. Because there is only one predictor, the covariance matrix $U$ simply reduces to the variance of the estimate.

The summary output above displays diagnostics for the linear regression model. The table uses aliases for concepts covered in the background section and introduces some new terminology. They are:

* `coefs` - The parameter estimate $\hat Q$
* `std` - The standard error of the coefficient estimate
* `vw` - The variance-within $\bar U$
* `vb` - The variance-between $B$
* `vt` - The total variance $T$
* `dfcom` - The degrees of freedom for the hypothetically complete dataset 
* `dfadj` - Adapted degrees of freedom for samples that have missing data
* `lambda` - The proportion of variance due to nonresponse or missingness
* `riv` - The relative increase in variance due to nonresponse or missingness

Observe that `vb`, `lambda`, and `riv` are all equal to 0. This result occurs because no missingness exists and multiple imputation is not used. These diagnostics are of interest for methods that produce variability in imputed values when using mutliple imputation. Therefore, values for `coefs`, `std`, and `vw` are the values of interest. They are the benchmarks for comparison when the authors reproduce this analysis on variants of the full dataset that have artificial missingness introduced. The rest of the diagnostics will become clear in the next sections.

## Example 1: MCAR with Missingness in the Response {.unnumbered}

In the first example, the researchers generate MCAR missingness within the response, $y$. Response $y$ contains 40% missing values. Feature $x$ remains fully observed. The two plots below showcase how to use `Autoimpute` to explore missingness within a given dataset. The plots are quite simple in this case, but they can help detect patterns in missing data when multiple features are present with different levels of missingness.

```{r y-mis-forty-loc, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Missingness Locations"}
include_graphics(path = "figure/y-mis-forty-loc.jpg")
```

```{r y-mis-forty-bar, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Missingness Percentage"}
include_graphics(path = "figure/y-mis-forty-bar.jpg")
```

Next, the researchers employ missing data methods to handle missing data. In this case, missing data methods must find plausible imputations for the 40% of $y$ that is missing. The imputation methods used include mean, linear regression, and pmm. The researchers also use listwise deletion, although there is no visualization within `Autoimpute` for complete-case analysis because no imputations are performed. For imputation methods, the researchers deploy each strategy within the multiple imputation framework. The number of imputations performed for each method is 5.

The visualizations below show the impact of mean, linear regression, and pmm. For each strategy, there are two respective plots. The first plot is the new multivariate distribution between $x$ and $y$ after imputation. The second plot is a swarm plot that shows the imputations for $y$ against other, observed values for $y$ for each of the $5$ imputations performed. 

Let's start with mean imputation:

```{r multi-mean, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Joint and Marginals with Mean Imputation"}
include_graphics(path = "figure/multi-mean.jpg")
```

```{r swarm-mean, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Swarm Plot: Mean Imputation"}
include_graphics(path = "figure/swarm-mean.jpg")
```

Note that for mean imputation, the imputations do not depend on the value of $x$, and the relationship between $y$ and $x$ is ignored. This result makes sense, as mean imputation is a univariate method. Also note that the imputation values in the swarm plot are the same for each imputation. This occurs because the mean of the observed values of $y$ do not change from imputation to imputation within the multiple imputation framework. Therefore, the imputation values within each imputed dataset are the same, and the imputation values accross each imputed dataset are the same.

Next, observe imputation via least squares:

```{r multi-lm, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Joint and Marginals with Least Squares Imputation"}
include_graphics(path = "figure/multi-lm.jpg")
```

```{r swarm-lm, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Swarm Plot: Least Squares Imputation"}
include_graphics(path = "figure/swarm-lm.jpg")
```

The plots for linear regression now take into account the relationship between $y$ and $x$. As a result, the imputed values are different within imputations but the same across imputations. Within imputations, the linear model produces different results, which depend on the value of $x$. But across imputations, the linear model is the same because it is fit to the same data and no random error is added to imputations.

Finally, observe pmm imputation:

```{r multi-pmm, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Joint and Marginals with PMM Imputation"}
include_graphics(path = "figure/multi-pmm.jpg")
```

```{r swarm-pmm, echo=FALSE, out.width="300px", fig.align="center", fig.cap="Swarm Plot: PMM Imputation"}
include_graphics(path = "figure/swarm-pmm.jpg")
```

PMM Imputation respects not only the relationship between $y$ and $x$ but also the variance between the features. Imputations are no longer from the "line of best fit" as they are with linear regression. Additionally, imputations are different within and across imputations. Therefore, PMM does the best job at respecting the structure of the data and adding variance between / across imputed datasets.

## Example 2: MAR with Missingness in the Predictor {.unnumbered}

In the second example, the researchers generate MAR missingness within the predictor, $x$. Predictor $x$ contains 40% missing values. Response $y$ remains fully observed. To keep this section concise, we will not generate the same plots that we did above, but we will apply the exact same methods. `Autoimpute` can impute both features and predictors, and it can examine missingness anywhere it exists within a dataset.

The researchers take the same approach to Example 2 as they do to Example 1. Multiple imputation is performed, with the number of imputations equal to 5. The same methods are applied as well (listwise delete, mean, least squares, and pmm).

## Analysis Models on each Example {.unnumbered}

The sections above take the user through the data exploration phase and multiple imputation phase of `Autoimpute`. The next section, Findings, demonstrates how the nature of missingness affected the results of linear regression on our mulitply imputed data.
