# Methodology {#methodology .unnumbered}

The missing data methods introduced in the background section are a subset of those available in `Autoimpute`. Beyond `Autoimpute` numerous additional missing data methods exist to handle missing data, and they are continuously being developed and improved (Schouten, et al, 2018). Therefore, it is vital to understand the circumstances under which each missing data method is effective, and it is important to evaluate each technique in relation to other methods (Schouten, et al, 2018). To do so, researchers can simulate complete datasets then introduce artificial missingness under different types of missing data mechanisms with different missing data patterns. Such a procedure affords the researcher an opportunity to evaluate missing data methods and their effect on analysis models under different circumstances (Schouten, et al, 2018).

## Assessing Missing Data Methods through Simulation {#methodology .unnumbered}

Schouten, et al. (2018) formalizes a step-by-step process in which a missing data methodology is evaluated by means of simulations:

1. Simulate a multivariate, complete dataset. The complete dataset becomes the population of interest and the source of truth.
2. Introduce arificial missingness to the complete dataset using missing data mechanisms and missing data patterns. This process results in an incomplete dataset. 
3. Use missing data methods to handle the missing data in the incomplete dataset.
4. Apply an analysis model and obtain statistical inference for the original, complete dataset as well as the incomplete dataset after dealing with the missing values. A comparison of these inferences gives an indication of the performance of the missing data method.

In this methodology, the authors follow the steps outlined above. The authors simulate a complete dataset using the `MASS` package in R (Venables & Ripley, 2002). The authors introduce missingness in a complete dataset using the `ampute` function from the `MICE` package in R (van Buuren & Groothuis-Oudshoorn, 2011). The `ampute` function can generate multivariate missing data within a complete dataset by defining a missing data mechanism (MCAR, MAR, MNAR) as well as missing data patterns, such as proportions of missingness and the features that should contain missingness (van Buuren & Groothuis-Oudshoorn, 2011). The authors use these R packages to produce complete and incomplete datasets, which cover steps 1 and 2 above.

Next, the authors use `Autoimpute` to cover steps 3 and 4. The authors explore the performance of imputation methods and their effect on analysis models under two separate circumstances. The authors begin by simulating a dataset with no missingness. A simple linear regression is performed on this simulated dataset and its parameters of interest are stored as benchmarks. Next, the researchers create artifical missingness in the complete dataset using two different types of missing data mechanisms with different missing data patterns. This procedure results in two disctinct, incomplete datasets. Each incomplete dataset will have different observations missing, but any observed values will be the same across the two incomplete datasets and the original complete dataset. 

The authors then impose the same deletion and impution methods on each of the incomplete datasets. In each example, the researchers first explore missingness patterns within the dataset. Then, the authors use the following missing data methods:

* Listwise deletion or CCA
* Mean imputation
* Least Squares imputation
* Predictive Mean Matching

Mean, least squares, and predictive mean matching are performed using the mutliple imputation framework. Listwise deletion is performed separately because it does not require single or multiple imputation. All methods are implemented using the `Autoimpute` package.

`Autoimpute` offers many more missing data methods, but these four are useful for this study because each carries a different set of assumptions and takes a very different approach to handling missing values. Listwise deletion is a deletion method, so it ignores missingness entirely by discarding records with any missing observations. Mean imputation is a univariate method, so it disregards any structure between the target variable and other features. Least squares is a supervised multivariate method that considers the structure of the data but ignores the uncertainty surrounding imputation. Finally, PMM is an advanced semi-supervised multivariate method that blends concepts from bayesian analysis, linear regression, and k-nearest neighbors. The sections below demonstrate when these methods work and when they do not. As always, their performance is directly linked to the missing data mechanism at hand and the nature of missingness within a dataset. Therefore, some methods shine in circumstances where other methods underperform.

## The Full Dataset {.unnumbered}

The authors use the MASS package (Venables & Ripley, 2002) to simulate the full dataset, or the source of truth. The full dataset contains 500 observations for feature $x$ and response $y$. The dataset is generated from a joint normal distribution, where $\mu_x=10$ and $\mu_y=5$. The correlation between $x$ and $y$ equals 0.7. At this point, no missingness exists in $x$ or $y$.

The figure below visualizes the distribution of each variable:

```{r fullsidebyside, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Distribution of Full x and y"}
include_graphics(path = "figure/full-side-by-side.jpg")
```

The next figure displays the joint relationship between $x$ and $y$. The figure plots the marginal distribution of each variable as well:

```{r full-joint, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals of Full x and y"}
include_graphics(path = "figure/full-joint.jpg")
```

Both plots come from native visualization methods in the `Autoimpute` package.

The full dataset does not contain any missing values, so no deletion or imputation is necessary before conducting analysis. Using `Autoimpute`, the researchers fit a linear regression model on the full dataset. The summary of the model fit appears below:

```{r full-regression, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Linear Regression Results: Full"}
include_graphics(path = "figure/full-regression.jpg")
```

The results from linear regression on the full dataset serve as the golden source. The coefficient estimate for $x$ is 0.7. Using notation related to imputation analysis, this coefficient estimate of $x$ represents $\hat Q$ - the unbiased estimate of the population parameter of interest $Q$. Because there is only one predictor, the covariance matrix $U$ simply reduces to the variance of the estimate.

The summary output above displays diagnostics for the linear regression model. The output uses aliases for concepts covered in the [Background section](#background)

* `coefs` - The parameter estimate $\hat Q$
* `std` - The standard error of the coefficient estimate
* `vw` - The variance-within $\bar U$
* `vb` - The variance-between $B$
* `vt` - The total variance $T$

Observe that `vb`, or the variance-between, is equal to 0. This result occurs because no missingness exists and multiple imputation is not used. The variance-between becomes important in subsequent sections where missing data methods are used within the multiple imputation framework.

## Example 1: MCAR with Missingness in the Response {.unnumbered}

In the first example, the researchers generate MCAR missingness within the response, $y$. Response $y$ contains 40% missing values. Feature $x$ remains fully observed. The two plots below showcase how to use `Autoimpute` to explore missingness within a given dataset. `Autoimpute` leverages the excellent `missingno` Python package to explore missingness within datasets (Bilogur, 2018) prior to applying any missing data methods. The plots are quite simple in this case, but they can help detect patterns in missing data when multiple features are present with different levels of missingness.

```{r y-mis-forty-loc, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Missingness Locations, MCAR"}
include_graphics(path = "figure/y-mis-forty-loc.jpg")
```

```{r y-mis-forty-bar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Missingness Percentage, MCAR"}
include_graphics(path = "figure/y-mis-forty-bar.jpg")
```

Next, the researchers employ missing data methods to handle missing data in $y$. In this case, missing data methods must find plausible imputations for the 40% of $y$ that is missing. The imputation methods used include mean, linear regression, and pmm. The researchers also use listwise deletion, although there is no visualization within `Autoimpute` for complete-case analysis because no imputations are performed. For imputation methods, the researchers deploy each strategy within the multiple imputation framework. The number of imputations performed for each method is 5.

`Autoimpute` natively supports a number of visualization methods to assess the impact of imputation on incomplete datasets. 

The visualizations below show the impact of mean, linear regression, and pmm imputation. For each strategy, there are two respective plots. The first plot is the new scatterplot between $x$ and $y$ after imputation. It also includes the new marginal distributions for $x$ and $y$ after imputation. The second plot shows the imputations for $y$ comingled with observed values for $y$ for each of the 5 imputations performed in the multiple imputation framework. 

Let's start with mean imputation:

```{r multi-mean, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with Mean Imputation, MCAR"}
include_graphics(path = "figure/multi-mean.jpg")
```

```{r swarm-mean, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: Mean Imputation, MCAR"}
include_graphics(path = "figure/swarm-mean.jpg")
```

Note that for mean imputation, the imputations do not depend on the value of $x$, and the relationship between $y$ and $x$ is ignored. This result makes sense, as mean imputation is a univariate method. Also note that the imputation values in the swarm plot are the same for each imputation. This occurs because the mean of the observed values of $y$ do not change from imputation to imputation within the multiple imputation framework. Therefore, the imputation values within each imputed dataset are the same, and the imputation values across each imputed dataset are the same.

Next, observe imputation via least squares:

```{r multi-lm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with Least Squares Imputation, MCAR"}
include_graphics(path = "figure/multi-lm.jpg")
```

```{r swarm-lm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: Least Squares Imputation, MCAR"}
include_graphics(path = "figure/swarm-lm.jpg")
```

The plots for linear regression now take into account the relationship between $y$ and $x$. As a result, the imputed values are different within an imputed dataset but the same across imputed datasets. Within an incomplete dataset, the linear model produces different imputed values, which depend on the value of $x$. But across incomplete datasets, the linear model's imputed values are the same because the model is fit on the same observed data and no random error is added to imputations.

Finally, observe PMM imputation:

```{r multi-pmm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with PMM Imputation, MCAR"}
include_graphics(path = "figure/multi-pmm.jpg")
```

```{r swarm-pmm, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: PMM Imputation, MCAR"}
include_graphics(path = "figure/swarm-pmm.jpg")
```

PMM Imputation respects not only the relationship between $y$ and $x$ but also the variance between the features. Imputations are no longer from the "line of best fit" as they are with least squares. Additionally, imputations are different within and across imputed datasets. Therefore, PMM does the best job at respecting the structure of the data and producing variability between and across imputed datasets to account for the uncertainty of the true value of a missing data point.

## Example 2: MAR with Missingness in the Predictor {.unnumbered}

In the second example, the researchers generate MAR missingness within the predictor, $x$. The missigness is right-tailed, which means that the probability that predictor $x$ is missing increases with the value of $y$. Predictor $x$ contains 40% missing values. Response $y$ remains fully observed.

The researchers take the same approach to Example 2 as they do to Example 1. First, the authors plot the missing data patterns within $x$ and $y$. Then, listwise deletion is performed, followed by multiple imputation for mean, least squares, and predictive mean matching. Again, the multiple imputation iterates 5 times.

The plots below demonstrate that missingness now occurs within the predictor $x$, while $y$ is fully observed.

```{r x-mis-forty-loc-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Missingness Locations, MAR"}
include_graphics(path = "figure/x-mis-forty-loc-mar.jpg")
```

```{r x-mis-forty-bar-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Missingness Percentage, MAR"}
include_graphics(path = "figure/x-mis-forty-bar-mar.jpg")
```

Next, the authors plot the scatterplot & marginals for each imputation method as well as the resulting swarm plot.

Starting with mean:

```{r multi-mean-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with Mean Imputation, MAR"}
include_graphics(path = "figure/multi-mean-mar.jpg")
```

```{r swarm-mean-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: Mean Imputation, MAR"}
include_graphics(path = "figure/swarm-mean-mar.jpg")
```

Now, mean imputations do not depend on the value of $y$. The relationship between $y$ and $x$ is still ignored. This result makes sense, as mean imputation is a univariate method. Also note that the imputation values in the swarm plot are the same for each imputation. This occurs because the mean of the observed values of $x$ do not change from imputation to imputation within the multiple imputation framework. Therefore, the imputation values within each imputed dataset are the same, and the imputation values across each imputed dataset are the same. Lastly, observe that mean imputation does not care which missing data mechanism is present because it considers only the target variable it imputes.

Then least squares:

```{r multi-lm-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with Least Squares Imputation, MAR"}
include_graphics(path = "figure/multi-lm-mar.jpg")
```

```{r swarm-lm-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: Least Squares Imputation, MAR"}
include_graphics(path = "figure/swarm-lm-mar.jpg")
```

The plots for linear regression still follow the logic explained from the plots in Example 1. That being said, the missing data mechanism now clearly affects the resulting imputations. Remember, the imputed values from linear regression come from the line of best fit of the imputation model. When the missing data mechanism changes and the missingness appears in predictor $x$, the slope of the line formed by the imputations is drastically different than it is in Example 1. This observation becomes important later on when the authors examine the effect of least squares imputation on analysis models from each example.

And finally PMM:

```{r multi-pmm-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Joint and Marginals with PMM Imputation, MAR"}
include_graphics(path = "figure/multi-pmm-mar.jpg")
```

```{r swarm-pmm-mar, echo=FALSE, out.width="400px", fig.align="center", fig.cap="Swarm Plot: PMM Imputation, MAR"}
include_graphics(path = "figure/swarm-pmm-mar.jpg")
```

PMM also follows the logic explained from the plots in Example 1. But now, PMM takes into account the missing data mechanism at hand. Imputations for $x$ are more often larger values than smaller ones. This is because there are more missing values in $x$ for higher values of $y$, and $x$ and $y$ are positively correlated. Again, this observation becomes important later on when the authors examine the effect of PMM imputation on analysis models from each example.

## Analysis Models on each Example {.unnumbered}

The sections above take the user through the data exploration phase and multiple imputation phase of `Autoimpute`. At each step, the authors use visualization methods from the `Autoimpute` package to demonstrate what each imputation method does under the hood when used in the multiple imputation framework. The next section, [Findings](#findings), demonstrates how each imputation algorithm affects the inference derived from pooled coefficients of linear regression applied to multiply imputed data. Results from analysis on imputed datasets are compared to the results from analysis on the complete dataset.
